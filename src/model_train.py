"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ EfficientNet-B4 —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º Gap –∏ Early Stopping.
"""

import torch
import torch.nn as nn
from datetime import datetime
import os
import json


class ModelTrainer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–∞–±–ª–µ—Ç–æ–∫
    
    Args:
        model: –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        train_loader: DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        criterion: —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        optimizer: –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        scheduler: scheduler –¥–ª—è learning rate
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cuda/cpu)
        save_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
    """
    def __init__(self, model, train_loader, val_loader, criterion, optimizer, 
                 scheduler, device, save_dir):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.device = device
        self.save_dir = save_dir
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        self.history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': [],
            'gap': [],
            'lr': []
        }
        
        # –õ—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        self.best_val_acc = 0.0
        self.best_val_loss = float('inf')
        self.best_gap = float('inf')
        
        # Early stopping
        self.patience = 0
        self.patience_limit = 10
        
        # –°–æ–∑–¥–∞—ë–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        os.makedirs(save_dir, exist_ok=True)
    
    def train_one_epoch(self, epoch):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –æ–¥–Ω—É —ç–ø–æ—Ö—É
        
        Args:
            epoch (int): –Ω–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π —ç–ø–æ—Ö–∏
            
        Returns:
            tuple: (train_loss, train_acc)
        """
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for batch_idx, (images, labels) in enumerate(self.train_loader):
            images, labels = images.to(self.device), labels.to(self.device)
            
            # Forward pass
            self.optimizer.zero_grad()
            outputs = self.model(images)
            loss = self.criterion(outputs, labels)
            
            # Backward pass
            loss.backward()
            self.optimizer.step()
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –≤—ã–≤–æ–¥ –∫–∞–∂–¥—ã–µ 10 –±–∞—Ç—á–µ–π
            if (batch_idx + 1) % 10 == 0:
                batch_acc = 100 * correct / total
                print(f"  –ë–∞—Ç—á {batch_idx+1}/{len(self.train_loader)}: "
                      f"Loss={loss.item():.4f}, Acc={batch_acc:.2f}%")
        
        train_loss = running_loss / len(self.train_loader)
        train_acc = 100 * correct / total
        
        return train_loss, train_acc
    
    def validate(self):
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        
        Returns:
            tuple: (val_loss, val_acc)
        """
        self.model.eval()
        running_vloss = 0.0
        vcorrect = 0
        vtotal = 0
        
        with torch.no_grad():
            for images, labels in self.val_loader:
                images, labels = images.to(self.device), labels.to(self.device)
                
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                
                running_vloss += loss.item()
                _, predicted = torch.max(outputs, 1)
                vtotal += labels.size(0)
                vcorrect += (predicted == labels).sum().item()
        
        val_loss = running_vloss / len(self.val_loader)
        val_acc = 100 * vcorrect / vtotal
        
        return val_loss, val_acc
    
    def save_checkpoint(self, epoch, is_best=False):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç checkpoint –º–æ–¥–µ–ª–∏
        
        Args:
            epoch (int): –Ω–æ–º–µ—Ä —ç–ø–æ—Ö–∏
            is_best (bool): —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –ª—É—á—à–µ–π –º–æ–¥–µ–ª—å—é
        """
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_acc': self.history['val_acc'][-1],
            'val_loss': self.history['val_loss'][-1],
            'gap': self.history['gap'][-1],
            'history': self.history,
            'best_val_acc': self.best_val_acc,
            'best_val_loss': self.best_val_loss,
            'best_gap': self.best_gap
        }
        
        if is_best:
            model_path = os.path.join(self.save_dir, 'best_efficientnet_b4.pt')
            torch.save(checkpoint, model_path)
            return model_path
        else:
            model_path = os.path.join(self.save_dir, f'checkpoint_epoch_{epoch}.pt')
            torch.save(checkpoint, model_path)
            return model_path
    
    def train(self, epochs=40):
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            epochs (int): –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            
        Returns:
            dict: –∏—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        """
        print(f"\n{'='*60}")
        print("–û–ë–£–ß–ï–ù–ò–ï EfficientNet-B4")
        print(f"{'='*60}")
        print(f"–ù–∞—á–∞–ª–æ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"–≠–ø–æ—Ö: {epochs}")
        print(f"Early Stopping: {self.patience_limit} —ç–ø–æ—Ö")
        print(f"{'='*60}\n")
        
        for epoch in range(epochs):
            print(f"{'='*60}")
            print(f"–≠–ø–æ—Ö–∞ {epoch+1}/{epochs}")
            print(f"{'='*60}")
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss, train_acc = self.train_one_epoch(epoch)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            val_loss, val_acc = self.validate()
            
            # –ú–µ—Ç—Ä–∏–∫–∏
            gap = train_acc - val_acc
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            self.history['train_loss'].append(train_loss)
            self.history['train_acc'].append(train_acc)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            self.history['gap'].append(gap)
            self.history['lr'].append(current_lr)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ Learning Rate
            self.scheduler.step()
            
            # –í—ã–≤–æ–¥
            gap_status = "‚úÖ" if gap < 10 else "‚ö†Ô∏è" if gap < 15 else "üî¥"
            
            print(f"\n{'‚îÄ'*60}")
            print(f"–ò–¢–û–ì–ò –≠–ü–û–•–ò {epoch+1}")
            print(f"{'‚îÄ'*60}")
            print(f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
            print(f"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%")
            print(f"{gap_status} Gap:      {gap:.2f}%")
            print(f"Learning Rate: {current_lr:.7f}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            improved = False
            
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self.best_val_loss = val_loss
                self.best_gap = gap
                self.patience = 0
                improved = True
                
                model_path = self.save_checkpoint(epoch, is_best=True)
                print(f"‚úì –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
                print(f"  Val Acc: {val_acc:.2f}%, Gap: {gap:.2f}%")
            
            elif val_acc == self.best_val_acc and gap < self.best_gap:
                self.best_gap = gap
                self.patience = 0
                improved = True
                
                model_path = self.save_checkpoint(epoch, is_best=True)
                print(f"‚úì –ú–æ–¥–µ–ª—å —Å –º–µ–Ω—å—à–∏–º Gap —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
                print(f"  Val Acc: {val_acc:.2f}%, Gap: {gap:.2f}%")
            
            else:
                self.patience += 1
                print(f"–ë–µ–∑ —É–ª—É—á—à–µ–Ω–∏–π: {self.patience}/{self.patience_limit}")
            
            # Early stopping
            if self.patience >= self.patience_limit:
                print(f"\n{'='*60}")
                print(f"‚èπ EARLY STOPPING –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
                print(f"–ù–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π {self.patience_limit} —ç–ø–æ—Ö –ø–æ–¥—Ä—è–¥")
                print(f"{'='*60}")
                break
            
            # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
            if gap > 20:
                print(f"\nüö® –í–ù–ò–ú–ê–ù–ò–ï: Gap > 20% - –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ!")
            
            if val_acc >= 75:
                print(f"\nüéØ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ü–µ–ª—å 75%!")
            
            print()
        
        # –ò—Ç–æ–≥–∏ –æ–±—É—á–µ–Ω–∏—è
        self._print_summary()
        
        return self.history
    
    def _print_summary(self):
        """–í—ã–≤–æ–¥–∏—Ç –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—É—á–µ–Ω–∏—è"""
        print(f"\n{'='*60}")
        print("–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û")
        print(f"{'='*60}")
        print(f"–û–∫–æ–Ω—á–∞–Ω–∏–µ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"–û–±—É—á–µ–Ω–æ —ç–ø–æ—Ö: {len(self.history['train_acc'])}")
        print(f"\n–õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"  Val Accuracy:  {self.best_val_acc:.2f}%")
        print(f"  Val Loss:      {self.best_val_loss:.4f}")
        print(f"  Gap:           {self.best_gap:.2f}%")
        
        if self.best_val_acc >= 75:
            print(f"\nüéâ –¶–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞! (>= 75%)")
        if self.best_gap < 10:
            print(f"üèÜ –û—Ç–ª–∏—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è (Gap < 10%)")
        elif self.best_gap < 15:
            print(f"‚úÖ –•–æ—Ä–æ—à–∞—è –≥–µ–Ω–µ—Ä–∞–ª–∏–∑–∞—Ü–∏—è (Gap < 15%)")
        
        print(f"\n–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {self.save_dir}/best_efficientnet_b4.pt")
        print(f"{'='*60}")
    
    def save_history(self, filename='training_history.json'):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è –≤ JSON
        
        Args:
            filename (str): –∏–º—è —Ñ–∞–π–ª–∞
        """
        history_path = os.path.join(self.save_dir, filename)
        
        history_data = {
            'history': self.history,
            'best_metrics': {
                'val_acc': self.best_val_acc,
                'val_loss': self.best_val_loss,
                'gap': self.best_gap
            },
            'training_info': {
                'total_epochs': len(self.history['train_acc']),
                'early_stopped': self.patience >= self.patience_limit
            }
        }
        
        with open(history_path, 'w') as f:
            json.dump(history_data, f, indent=4)
        
        print(f"‚úì –ò—Å—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {history_path}")


def train_model(model, train_loader, val_loader, device, save_dir, 
                epochs=40, lr=0.001, weight_decay=0.05, label_smoothing=0.15):
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è
    
    Args:
        model: –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        train_loader: DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        save_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        epochs (int): –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        lr (float): learning rate
        weight_decay (float): weight decay
        label_smoothing (float): label smoothing
        
    Returns:
        tuple: (trainer, history)
    """
    # –°–æ–∑–¥–∞—ë–º criterion
    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)
    
    # –°–æ–∑–¥–∞—ë–º optimizer
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=lr,
        weight_decay=weight_decay,
        betas=(0.9, 0.999)
    )
    
    # –°–æ–∑–¥–∞—ë–º scheduler
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=epochs,
        eta_min=1e-6
    )
    
    # –°–æ–∑–¥–∞—ë–º trainer
    trainer = ModelTrainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=criterion,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        save_dir=save_dir
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    history = trainer.train(epochs=epochs)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
    trainer.save_history()
    
    return trainer, history


if __name__ == "__main__":
    print("–ú–æ–¥—É–ª—å train.py –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
    print("\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:")
    print("""
from src.model_efficientnet import create_model
from src.prepare_dataloaders import create_dataloaders
from src.train import train_model
from config import PATHS

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
train_loader, val_loader, classes = create_dataloaders(PATHS['extracted_data'])

# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = create_model(num_classes=len(classes), device=device)

# –û–±—É—á–µ–Ω–∏–µ
trainer, history = train_model(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    device=device,
    save_dir=PATHS['models_dir'],
    epochs=40,
    lr=0.001,
    weight_decay=0.05
)
    """)