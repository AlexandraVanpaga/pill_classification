"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.
–í–∫–ª—é—á–∞–µ—Ç —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å TTA –∏ –±–µ–∑, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
"""

import torch
import torch.nn.functional as F
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
import json
import os
from datetime import datetime


class ModelEvaluator:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        test_loader: DataLoader –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        classes: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–ª–∞—Å—Å–æ–≤
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (cuda/cpu)
        save_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    def __init__(self, model, test_loader, classes, device, save_dir):
        self.model = model
        self.test_loader = test_loader
        self.classes = classes
        self.device = device
        self.save_dir = save_dir
        
        os.makedirs(save_dir, exist_ok=True)
        
        self.model.eval()
    
    def evaluate_baseline(self):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –±–µ–∑ TTA (baseline)
        
        Returns:
            tuple: (labels_true, labels_predicted, accuracy)
        """
        print("="*60)
        print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ë–ï–ó TTA (BASELINE)")
        print("="*60)
        
        labels_predicted = []
        labels_true = []
        
        with torch.no_grad():
            for batch_idx, (images, labels) in enumerate(self.test_loader):
                images = images.to(self.device)
                
                outputs = self.model(images)
                _, predicted = torch.max(outputs, 1)
                
                labels_predicted.extend(predicted.cpu().numpy())
                labels_true.extend(labels.numpy())
                
                if (batch_idx + 1) % 5 == 0:
                    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –±–∞—Ç—á–µ–π: {batch_idx + 1}/{len(self.test_loader)}")
        
        accuracy = accuracy_score(labels_true, labels_predicted)
        
        print(f"\n{'='*60}")
        print(f"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ë–ï–ó TTA")
        print(f"{'='*60}")
        print(f"Test Accuracy: {accuracy*100:.2f}%")
        
        if accuracy >= 0.75:
            print("\nüéâ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê –ë–ï–ó TTA!")
        else:
            gap = 75 - accuracy*100
            print(f"\n–î–æ —Ü–µ–ª–∏: {gap:.2f}%")
        
        print(f"{'='*60}\n")
        
        return labels_true, labels_predicted, accuracy
    
    def evaluate_with_tta(self, n_augmentations=4):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ —Å Test-Time Augmentation
        
        Args:
            n_augmentations (int): –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–π (4 –∏–ª–∏ 10)
            
        Returns:
            tuple: (labels_true, labels_predicted, accuracy)
        """
        print("="*60)
        print(f"–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –° TTA ({n_augmentations} –ê–£–ì–ú–ï–ù–¢–ê–¶–ò–ô)")
        print("="*60)
        
        if n_augmentations == 4:
            print("–ü—Ä–∏–º–µ–Ω—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏:")
            print("  1. –û—Ä–∏–≥–∏–Ω–∞–ª")
            print("  2. Horizontal flip ‚Üî")
            print("  3. Vertical flip ‚Üï")
            print("  4. Both flips ‚§°\n")
        
        labels_predicted = []
        labels_true = []
        
        with torch.no_grad():
            for batch_idx, (images, labels) in enumerate(self.test_loader):
                batch_predictions = []
                
                # 1. –û—Ä–∏–≥–∏–Ω–∞–ª
                imgs = images.to(self.device)
                outputs = self.model(imgs)
                probs = F.softmax(outputs, dim=1)
                batch_predictions.append(probs.cpu().numpy())
                
                # 2. Horizontal flip
                imgs_hflip = torch.flip(images, dims=[3]).to(self.device)
                outputs = self.model(imgs_hflip)
                probs = F.softmax(outputs, dim=1)
                batch_predictions.append(probs.cpu().numpy())
                
                # 3. Vertical flip
                imgs_vflip = torch.flip(images, dims=[2]).to(self.device)
                outputs = self.model(imgs_vflip)
                probs = F.softmax(outputs, dim=1)
                batch_predictions.append(probs.cpu().numpy())
                
                # 4. Both flips
                imgs_both = torch.flip(images, dims=[2, 3]).to(self.device)
                outputs = self.model(imgs_both)
                probs = F.softmax(outputs, dim=1)
                batch_predictions.append(probs.cpu().numpy())
                
                # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                avg_probs = np.mean(batch_predictions, axis=0)
                predicted = np.argmax(avg_probs, axis=1)
                
                labels_predicted.extend(predicted)
                labels_true.extend(labels.numpy())
                
                if (batch_idx + 1) % 5 == 0:
                    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {batch_idx + 1}/{len(self.test_loader)}")
        
        accuracy = accuracy_score(labels_true, labels_predicted)
        
        print(f"\n{'='*60}")
        print(f"–†–ï–ó–£–õ–¨–¢–ê–¢–´ –° TTA")
        print(f"{'='*60}")
        print(f"Test Accuracy: {accuracy*100:.2f}%")
        
        if accuracy >= 0.75:
            print("\nüéâüéâüéâ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê –° TTA! üéâüéâüéâ")
        else:
            gap = 75 - accuracy*100
            print(f"\n–î–æ —Ü–µ–ª–∏: {gap:.2f}%")
        
        print(f"{'='*60}\n")
        
        return labels_true, labels_predicted, accuracy
    
    def print_classification_report(self, labels_true, labels_predicted, title="Classification Report"):
        """
        –í—ã–≤–æ–¥–∏—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π classification report
        
        Args:
            labels_true: –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            labels_predicted: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            title: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—á—ë—Ç–∞
        """
        print(f"\n{title}:\n")
        print(classification_report(labels_true, labels_predicted, 
                                   target_names=self.classes, digits=3))
    
    def plot_confusion_matrix(self, labels_true, labels_predicted, filename='confusion_matrix.png'):
        """
        –°—Ç—Ä–æ–∏—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫
        
        Args:
            labels_true: –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            labels_predicted: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            filename: –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        conf_matrix = confusion_matrix(labels_true, labels_predicted)
        
        # –ë–µ—Ä—ë–º —Ç–æ–ø-20 –∫–ª–∞—Å—Å–æ–≤
        class_counts = conf_matrix.sum(axis=1)
        top_indices = np.argsort(class_counts)[-20:]
        
        conf_matrix_top = conf_matrix[top_indices][:, top_indices]
        classes_top = [self.classes[i] for i in top_indices]
        
        plt.figure(figsize=(16, 14))
        sns.heatmap(
            conf_matrix_top, 
            annot=True, 
            fmt='d', 
            cmap='Blues',
            xticklabels=classes_top,
            yticklabels=classes_top,
            cbar_kws={'label': 'Count'}
        )
        
        accuracy = accuracy_score(labels_true, labels_predicted)
        plt.title(f'Confusion Matrix - Top 20 Classes\nTest Accuracy: {accuracy*100:.2f}%', 
                 fontsize=14, fontweight='bold')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        save_path = os.path.join(self.save_dir, filename)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"‚úì –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")
    
    def save_results(self, baseline_acc, tta_acc, labels_true_tta, labels_predicted_tta):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
        
        Args:
            baseline_acc: accuracy –±–µ–∑ TTA
            tta_acc: accuracy —Å TTA
            labels_true_tta: –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (TTA)
            labels_predicted_tta: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ (TTA)
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = {
            'model': 'EfficientNet-B4',
            'timestamp': timestamp,
            'test_baseline': {
                'accuracy': float(baseline_acc * 100),
                'method': 'Single prediction'
            },
            'test_tta': {
                'accuracy': float(tta_acc * 100),
                'method': 'TTA (4 augmentations)',
                'augmentations': ['original', 'h_flip', 'v_flip', 'both_flips']
            },
            'improvement': {
                'tta_boost': float((tta_acc - baseline_acc) * 100)
            },
            'goal_achieved': tta_acc >= 0.75,
            'dataset': {
                'num_classes': len(self.classes),
                'test_samples': len(labels_true_tta)
            }
        }
        
        results_path = os.path.join(self.save_dir, 'test_results.json')
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=4, ensure_ascii=False)
        
        print(f"‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
        
        # Classification report
        report_dict = classification_report(
            labels_true_tta, 
            labels_predicted_tta, 
            target_names=self.classes,
            output_dict=True
        )
        
        report_path = os.path.join(self.save_dir, 'classification_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_dict, f, indent=4, ensure_ascii=False)
        
        print(f"‚úì Classification report: {report_path}")
        
        # –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        test_examples = []
        for i in range(min(100, len(labels_true_tta))):
            test_examples.append({
                'index': i,
                'true_label': self.classes[labels_true_tta[i]],
                'predicted_label': self.classes[labels_predicted_tta[i]],
                'correct': bool(labels_true_tta[i] == labels_predicted_tta[i])
            })
        
        examples_path = os.path.join(self.save_dir, 'test_examples.json')
        with open(examples_path, 'w', encoding='utf-8') as f:
            json.dump(test_examples, f, indent=4, ensure_ascii=False)
        
        print(f"‚úì –ü—Ä–∏–º–µ—Ä—ã: {examples_path}")
    
    def full_evaluation(self):
        """
        –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏: baseline + TTA + —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Returns:
            dict: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        # Baseline
        labels_true_base, labels_pred_base, acc_base = self.evaluate_baseline()
        
        # TTA
        labels_true_tta, labels_pred_tta, acc_tta = self.evaluate_with_tta()
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
        print(f"{'='*60}")
        print(f"–°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        print(f"{'='*60}")
        print(f"–ë–ï–ó TTA:  {acc_base*100:.2f}%")
        print(f"–° TTA:    {acc_tta*100:.2f}%")
        print(f"{'='*60}")
        print(f"–£–ª—É—á—à–µ–Ω–∏–µ: {(acc_tta - acc_base)*100:+.2f}%")
        print(f"{'='*60}\n")
        
        # Classification report
        self.print_classification_report(labels_true_tta, labels_pred_tta, 
                                        "–ü–æ–¥—Ä–æ–±–Ω—ã–π Classification Report –° TTA")
        
        # Confusion matrix
        self.plot_confusion_matrix(labels_true_tta, labels_pred_tta, 
                                   'confusion_matrix_tta.png')
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.save_results(acc_base, acc_tta, labels_true_tta, labels_pred_tta)
        
        print(f"\n–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.save_dir}")
        
        return {
            'baseline_accuracy': acc_base,
            'tta_accuracy': acc_tta,
            'improvement': acc_tta - acc_base,
            'goal_achieved': acc_tta >= 0.75
        }


def evaluate_model(model, test_loader, classes, device, save_dir):
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏
    
    Args:
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        test_loader: DataLoader –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        classes: —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        save_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        dict: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
    """
    evaluator = ModelEvaluator(
        model=model,
        test_loader=test_loader,
        classes=classes,
        device=device,
        save_dir=save_dir
    )
    
    results = evaluator.full_evaluation()
    
    return results


def load_and_evaluate(model_path, model_class, num_classes, test_loader, 
                     classes, device, save_dir):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ checkpoint –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –µ—ë
    
    Args:
        model_path: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        model_class: –∫–ª–∞—Å—Å –º–æ–¥–µ–ª–∏
        num_classes: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        test_loader: DataLoader –¥–ª—è —Ç–µ—Å—Ç–æ–≤
        classes: —Å–ø–∏—Å–æ–∫ –∫–ª–∞—Å—Å–æ–≤
        device: —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        save_dir: –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
    Returns:
        dict: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
    """
    # –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å
    model = model_class(num_classes=num_classes).to(device)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    print(f"‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
    print(f"  –≠–ø–æ—Ö–∞: {checkpoint.get('epoch', 'N/A')}")
    print(f"  Val Accuracy: {checkpoint.get('val_acc', 'N/A'):.2f}%\n")
    
    # –û—Ü–µ–Ω–∫–∞
    results = evaluate_model(model, test_loader, classes, device, save_dir)
    
    return results


if __name__ == "__main__":
    print("–ú–æ–¥—É–ª—å evaluate.py –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
    print("\n–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:")
    print("""
from src.model_efficientnet import PillClassifierEfficientNetB4
from src.prepare_dataloaders import create_test_loader
from src.evaluate import load_and_evaluate
from config import PATHS

# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
test_loader, classes = create_test_loader(PATHS['extracted_data'])

# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
results = load_and_evaluate(
    model_path=PATHS['best_model'],
    model_class=PillClassifierEfficientNetB4,
    num_classes=len(classes),
    test_loader=test_loader,
    classes=classes,
    device=device,
    save_dir=PATHS['results_dir']
)
    """)